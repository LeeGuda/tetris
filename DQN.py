import tensorflow as tf
from collections import deque
import numpy as np
import random
import pygame
import os
import time
from multiprocessing import Process, Queue, Manager, cpu_count

AdamOptimizer = tf.keras.optimizers.Adam

# ì „ì—­ ìƒìˆ˜ ì„¤ì • (ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ê³µìœ )
STATE_SIZE = 13
ACTION_MAP = [(r, x) for r in range(4) for x in range(10)]
ACTION_SIZE = len(ACTION_MAP)
REPLAY_MEMORY_SIZE = 20000 
N_WORKERS = cpu_count() - 1 

# ğŸ’¡ ëª¨ë¸ ì €ì¥ ê²½ë¡œ ìƒìˆ˜ ì¶”ê°€
MODEL_SAVE_PATH = 'dqn_tetris_weights.weights.h5' 

class DQNAgent:
    """ì¤‘ì•™ ë° ëª¨ë‹ˆí„°ë§ ì—ì´ì „íŠ¸ë¡œ ì‚¬ìš©ë˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, state_size=STATE_SIZE, action_size=ACTION_SIZE):
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = 0.95
        self.learning_rate = 0.001
        
        self.model = self._build_model()
        self.target_model = self._build_model()
        self.update_target_model()

    def _build_model(self):
        """ì‹ ê²½ë§ ëª¨ë¸ êµ¬ì¶• (Keras ì‚¬ìš©). CPU ì¥ì¹˜ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•©ë‹ˆë‹¤."""
        with tf.device('/cpu:0'):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(64, activation='relu', input_shape=(self.state_size,)),
                tf.keras.layers.Dense(64, activation='relu'),
                tf.keras.layers.Dense(self.action_size, activation='linear')
            ])
            model.compile(loss='mse', optimizer=AdamOptimizer(learning_rate=self.learning_rate))
            return model

    def update_target_model(self):
        """ë©”ì¸ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ íƒ€ê²Ÿ ëª¨ë¸ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤."""
        self.target_model.set_weights(self.model.get_weights())
    
    def get_weights(self):
        """Worker/Monitorì—ê²Œ ì „ë‹¬í•  ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.model.get_weights()
    
    def set_weights(self, weights):
        """ê°€ì¤‘ì¹˜ë¥¼ ë°›ì•„ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        self.model.set_weights(weights)
    
    # ğŸ’¡ ê°€ì¤‘ì¹˜ ì €ì¥ ë©”ì„œë“œ ì¶”ê°€
    def save_weights(self, filename):
        """ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        # CPU ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ì €ì¥í•˜ì—¬ ë©€í‹°í”„ë¡œì„¸ì‹± í™˜ê²½ì—ì„œ ì•ˆì •ì„± í™•ë³´
        with tf.device('/cpu:0'):
            self.model.save_weights(filename)

    # ğŸ’¡ ê°€ì¤‘ì¹˜ ë¡œë“œ ë©”ì„œë“œ ì¶”ê°€
    def load_weights(self, filename):
        """íŒŒì¼ì—ì„œ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        with tf.device('/cpu:0'):
            self.model.load_weights(filename)
            self.update_target_model() # íƒ€ê²Ÿ ëª¨ë¸ë„ í•¨ê»˜ ì—…ë°ì´íŠ¸

    def act(self, state, possible_actions, ACTION_MAP, epsilon=0.0):
        """í–‰ë™ì„ ì„ íƒí•©ë‹ˆë‹¤."""
        # 1. íƒí—˜ 
        if np.random.rand() <= epsilon:
            random_action_tuple = random.choice(possible_actions)
            try:
                action_index = ACTION_MAP.index(random_action_tuple)
            except ValueError:
                action_index = random.randrange(self.action_size)
            return action_index
        
        # 2. í™œìš© (Exploitation): ìµœì  í–‰ë™ ì„ íƒ
        with tf.device('/cpu:0'):
            state_tensor = tf.convert_to_tensor(state[np.newaxis, :], dtype=tf.float32)
            q_values_tensor = self.model(state_tensor, training=False)
            q_values = q_values_tensor.numpy()[0]
        
        # ìœ íš¨í•œ í–‰ë™ë§Œ ê³ ë ¤í•˜ì—¬ Q ê°’ ë§ˆìŠ¤í‚¹
        possible_indices = {ACTION_MAP.index(act) for act in possible_actions if act in ACTION_MAP}
        
        for i in range(self.action_size):
            if i not in possible_indices:
                q_values[i] = -1e9  # ìœ íš¨í•˜ì§€ ì•Šì€ í–‰ë™ì€ ë¬´ì‹œ
                
        action_index = np.argmax(q_values)
        return action_index

    def replay(self, memory_queue, batch_size):
        """ê³µìœ  ë©”ëª¨ë¦¬ì—ì„œ ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ìƒ˜í”Œë§í•˜ê³  í•™ìŠµí•©ë‹ˆë‹¤."""
        if memory_queue.qsize() < batch_size:
            return

        batch = []
        while not memory_queue.empty() and len(batch) < batch_size:
            batch.append(memory_queue.get())
            
        if not batch: return

        states = np.array([e[0] for e in batch])
        action_indices = np.array([e[1] for e in batch])
        rewards = np.array([e[2] for e in batch])
        next_states = np.array([e[3] for e in batch])
        dones = np.array([e[4] for e in batch])
        
        with tf.device('/cpu:0'):
            next_q_values = self.target_model(next_states, training=False).numpy()
            targets = rewards + self.gamma * np.amax(next_q_values, axis=1) * (1 - dones.astype(int))
            
            target_f = self.model(states, training=False).numpy() 
            
            for i in range(len(batch)):
                target_f[i, action_indices[i]] = targets[i]
            
            self.model.train_on_batch(states, target_f)


def worker_process(worker_id, memory_queue, shared_weights, epsilon_map, global_steps, lock):
    """ì‘ì—…ì í”„ë¡œì„¸ìŠ¤ (Worker Agent)"""
    env = TetrisEnv(render_mode='none') 
    local_agent = DQNAgent()
    
    local_agent.set_weights(shared_weights)
    
    print(f"Worker {worker_id} started. Initial Epsilon: {epsilon_map['epsilon']:.4f}")
    
    while True:
        local_agent.set_weights(shared_weights)
        
        state = env.reset()
        done = False
        
        while not done:
            epsilon = epsilon_map['epsilon']
            
            possible_actions = env.get_possible_actions()
            action_index = local_agent.act(state, possible_actions, ACTION_MAP, epsilon=epsilon)
                
            action = ACTION_MAP[action_index]
            
            next_state, reward, done, _ = env.step(action)
            
            memory_queue.put((state, action_index, reward, next_state, done))
            
            with lock:
                global_steps['value'] += 1
            
            state = next_state
        
        with lock:
            if epsilon_map['epsilon'] > 0.01:
                epsilon_map['epsilon'] *= 0.995


def distributed_train_dqn(episodes=50000, batch_size=128, target_update_freq=10, render_freq=5, worker_count=N_WORKERS):
    
    # 1. ì¤‘ì•™ ì—ì´ì „íŠ¸ ë° ê³µìœ  ìì› ì„¤ì •
    global_agent = DQNAgent() 
    
    # ğŸ’¡ ì €ì¥ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
    if os.path.exists(MODEL_SAVE_PATH):
        print(f"Loading previous weights from {MODEL_SAVE_PATH}...")
        try:
            global_agent.load_weights(MODEL_SAVE_PATH)
            print("Weights successfully loaded. Resuming training.")
        except Exception as e:
            # ëª¨ë¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆê±°ë‚˜ íŒŒì¼ì´ ì†ìƒëœ ê²½ìš°
            print(f"Error loading weights ({e}). Starting training from scratch.")

    manager = Manager()
    memory_queue = Queue(maxsize=REPLAY_MEMORY_SIZE) 
    
    shared_weights = manager.list(global_agent.get_weights())
    epsilon_map = manager.dict({'epsilon': 1.0})
    global_steps = manager.dict({'value': 0})
    lock = manager.Lock()

    # 2. ë Œë”ë§ì„ ìœ„í•œ ë³„ë„ ëª¨ë‹ˆí„°ë§ ì—ì´ì „íŠ¸ ë° í™˜ê²½ ìƒì„±
    monitor_agent = DQNAgent() 
    monitor_env = TetrisEnv(render_mode='human')
    monitor_state = monitor_env.reset()
    monitor_done = False
    
    monitor_total_reward = 0.0
    monitor_step_count = 0
    
    # 3. Worker í”„ë¡œì„¸ìŠ¤ ìƒì„± ë° ì‹œì‘
    print(f"\n--- Starting Distributed Training with {worker_count} Workers (CPU Mode) ---")
    workers = []
    actual_worker_count = max(1, worker_count)
    for i in range(actual_worker_count):
        p = Process(target=worker_process, args=(i, memory_queue, shared_weights, epsilon_map, global_steps, lock))
        workers.append(p)
        p.start()

    # 4. ì¤‘ì•™ í•™ìŠµ ë£¨í”„ (ë©”ì¸ í”„ë¡œì„¸ìŠ¤)
    global_train_count = 0
    total_steps = 0
    
    while global_train_count < episodes:
        
        if memory_queue.qsize() < batch_size * 4: 
            print(f"Waiting for experience... Current size: {memory_queue.qsize()}", end='\r')
            time.sleep(1)
            continue
            
        # 4.1 ëª¨ë¸ í•™ìŠµ
        global_agent.replay(memory_queue, batch_size)
        global_train_count += 1
        
        # 4.2 íƒ€ê²Ÿ ëª¨ë¸ ì—…ë°ì´íŠ¸
        if global_train_count % target_update_freq == 0:
            global_agent.update_target_model()
        
        # 4.3 Workerë“¤ì—ê²Œ ì—…ë°ì´íŠ¸ëœ ê°€ì¤‘ì¹˜ ë™ê¸°í™”
        if global_train_count % 1 == 0: 
             new_weights = global_agent.get_weights()
             for i, w in enumerate(new_weights):
                 shared_weights[i] = w
        
        # ğŸ’¡ 4.4 ì£¼ê¸°ì ì¸ ëª¨ë¸ ì €ì¥ (1000 í•™ìŠµ ìŠ¤í…ë§ˆë‹¤)
        if global_train_count % 1000 == 0 and global_train_count > 0:
            print(f"\n--- Saving model weights at Train Step {global_train_count} ---")
            global_agent.save_weights(MODEL_SAVE_PATH)
        
        # 4.5 ì£¼ê¸°ì ì¸ ë Œë”ë§ ë° ëª¨ë‹ˆí„°ë§ (ì½˜ì†” ì¶œë ¥ìš©)
        if global_train_count % render_freq == 0: 
            monitor_agent.set_weights(global_agent.get_weights())
            
            if monitor_done:
                monitor_state = monitor_env.reset()
                monitor_total_reward = 0.0 
                monitor_step_count = 0
                monitor_done = False
            
            possible_actions = monitor_env.get_possible_actions()
            action_index = monitor_agent.act(monitor_state, possible_actions, ACTION_MAP, epsilon=0.0)
            action = ACTION_MAP[action_index]
            
            monitor_state, reward, monitor_done, _ = monitor_env.step(action)
            
            monitor_total_reward += reward
            monitor_step_count += 1
            
            # TetrisEnv.pyì˜ render í•¨ìˆ˜ëŠ” ì¸ìˆ˜ë¥¼ ë°›ì§€ ì•Šë„ë¡ ìˆ˜ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
            monitor_env.render()
            
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    print("\nUser quit signal received. Terminating training and saving model...")
                    # ğŸ’¡ ì¢…ë£Œ ì‹œ ìµœì¢… ê°€ì¤‘ì¹˜ ì €ì¥
                    global_agent.save_weights(MODEL_SAVE_PATH) 
                    monitor_env.close()
                    for p in workers: p.terminate(); p.join()
                    return

        # 4.6 í†µê³„ ì¶œë ¥ (ì½˜ì†”)
        total_steps = global_steps['value']
        if global_train_count % 10 == 0:
            print(f"Train Step: {global_train_count}/{episodes}, Steps: {total_steps}, Epsilon: {epsilon_map['epsilon']:.4f} | Monitor -> Steps: {monitor_step_count}, Reward: {monitor_total_reward:.2f}")

    # 5. ìµœì¢… ì¢…ë£Œ
    print("\n--- Distributed Training Finished. Finalizing and Saving Model Weights ---")
    global_agent.save_weights(MODEL_SAVE_PATH) # ìµœì¢… ê°€ì¤‘ì¹˜ ì €ì¥
    
    monitor_env.close()
    for p in workers:
        p.terminate()
        p.join()
        
    print("Training Complete.")

if __name__ == '__main__':
    pygame.init() 
    distributed_train_dqn(episodes=50000, batch_size=128, target_update_freq=10, render_freq=5, worker_count=N_WORKERS)